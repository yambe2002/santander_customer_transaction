{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold,StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/train.csv')\n",
    "# df = pd.read_csv('input/train_min.csv')  # small data\n",
    "test_df = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop('ID_code',axis=1)\n",
    "X = train_df.drop(['ID_code','target'],axis=1)\n",
    "y = train_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0656</td>\n",
       "      <td>7.7798</td>\n",
       "      <td>12.9536</td>\n",
       "      <td>9.4292</td>\n",
       "      <td>11.4327</td>\n",
       "      <td>-2.3805</td>\n",
       "      <td>5.8493</td>\n",
       "      <td>18.2675</td>\n",
       "      <td>2.1337</td>\n",
       "      <td>8.8100</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.1556</td>\n",
       "      <td>11.8495</td>\n",
       "      <td>-1.4300</td>\n",
       "      <td>2.4508</td>\n",
       "      <td>13.7112</td>\n",
       "      <td>2.4669</td>\n",
       "      <td>4.3654</td>\n",
       "      <td>10.7200</td>\n",
       "      <td>15.4722</td>\n",
       "      <td>-8.7197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.5304</td>\n",
       "      <td>1.2543</td>\n",
       "      <td>11.3047</td>\n",
       "      <td>5.1858</td>\n",
       "      <td>9.1974</td>\n",
       "      <td>-4.0117</td>\n",
       "      <td>6.0196</td>\n",
       "      <td>18.6316</td>\n",
       "      <td>-4.4131</td>\n",
       "      <td>5.9739</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6165</td>\n",
       "      <td>8.8349</td>\n",
       "      <td>0.9403</td>\n",
       "      <td>10.1282</td>\n",
       "      <td>15.5765</td>\n",
       "      <td>0.4773</td>\n",
       "      <td>-1.4852</td>\n",
       "      <td>9.8714</td>\n",
       "      <td>19.1293</td>\n",
       "      <td>-20.9760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.4827</td>\n",
       "      <td>-10.3581</td>\n",
       "      <td>10.1407</td>\n",
       "      <td>7.0479</td>\n",
       "      <td>10.2628</td>\n",
       "      <td>9.8052</td>\n",
       "      <td>4.8950</td>\n",
       "      <td>20.2537</td>\n",
       "      <td>1.5233</td>\n",
       "      <td>8.3442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7484</td>\n",
       "      <td>10.9935</td>\n",
       "      <td>1.9803</td>\n",
       "      <td>2.1800</td>\n",
       "      <td>12.9813</td>\n",
       "      <td>2.1281</td>\n",
       "      <td>-7.1086</td>\n",
       "      <td>7.0618</td>\n",
       "      <td>19.8956</td>\n",
       "      <td>-23.1794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.5374</td>\n",
       "      <td>-1.3222</td>\n",
       "      <td>12.0220</td>\n",
       "      <td>6.5749</td>\n",
       "      <td>8.8458</td>\n",
       "      <td>3.1744</td>\n",
       "      <td>4.9397</td>\n",
       "      <td>20.5660</td>\n",
       "      <td>3.3755</td>\n",
       "      <td>7.4578</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5702</td>\n",
       "      <td>9.0766</td>\n",
       "      <td>1.6580</td>\n",
       "      <td>3.5813</td>\n",
       "      <td>15.1874</td>\n",
       "      <td>3.1656</td>\n",
       "      <td>3.9567</td>\n",
       "      <td>9.2295</td>\n",
       "      <td>13.0168</td>\n",
       "      <td>-4.2108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.7058</td>\n",
       "      <td>-0.1327</td>\n",
       "      <td>14.1295</td>\n",
       "      <td>7.7506</td>\n",
       "      <td>9.1035</td>\n",
       "      <td>-8.5848</td>\n",
       "      <td>6.8595</td>\n",
       "      <td>10.6048</td>\n",
       "      <td>2.9890</td>\n",
       "      <td>7.1437</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2259</td>\n",
       "      <td>9.1723</td>\n",
       "      <td>1.2835</td>\n",
       "      <td>3.3778</td>\n",
       "      <td>19.5542</td>\n",
       "      <td>-0.2860</td>\n",
       "      <td>-5.1612</td>\n",
       "      <td>7.2882</td>\n",
       "      <td>13.9260</td>\n",
       "      <td>-9.1846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_0    var_1    var_2   var_3    var_4   var_5   var_6    var_7  \\\n",
       "0  11.0656   7.7798  12.9536  9.4292  11.4327 -2.3805  5.8493  18.2675   \n",
       "1   8.5304   1.2543  11.3047  5.1858   9.1974 -4.0117  6.0196  18.6316   \n",
       "2   5.4827 -10.3581  10.1407  7.0479  10.2628  9.8052  4.8950  20.2537   \n",
       "3   8.5374  -1.3222  12.0220  6.5749   8.8458  3.1744  4.9397  20.5660   \n",
       "4  11.7058  -0.1327  14.1295  7.7506   9.1035 -8.5848  6.8595  10.6048   \n",
       "\n",
       "    var_8   var_9   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "0  2.1337  8.8100   ...     -2.1556  11.8495  -1.4300   2.4508  13.7112   \n",
       "1 -4.4131  5.9739   ...     10.6165   8.8349   0.9403  10.1282  15.5765   \n",
       "2  1.5233  8.3442   ...     -0.7484  10.9935   1.9803   2.1800  12.9813   \n",
       "3  3.3755  7.4578   ...      9.5702   9.0766   1.6580   3.5813  15.1874   \n",
       "4  2.9890  7.1437   ...      4.2259   9.1723   1.2835   3.3778  19.5542   \n",
       "\n",
       "   var_195  var_196  var_197  var_198  var_199  \n",
       "0   2.4669   4.3654  10.7200  15.4722  -8.7197  \n",
       "1   0.4773  -1.4852   9.8714  19.1293 -20.9760  \n",
       "2   2.1281  -7.1086   7.0618  19.8956 -23.1794  \n",
       "3   3.1656   3.9567   9.2295  13.0168  -4.2108  \n",
       "4  -0.2860  -5.1612   7.2882  13.9260  -9.1846  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var_0   var_1    var_2   var_3    var_4   var_5   var_6    var_7   var_8  \\\n",
       "0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187  18.6266 -4.9200   \n",
       "1  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208  16.5338  3.1468   \n",
       "2   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427  14.6155 -4.9193   \n",
       "3  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428  14.9250 -5.8609   \n",
       "4   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405  19.2514  6.2654   \n",
       "\n",
       "    var_9   ...     var_190  var_191  var_192  var_193  var_194  var_195  \\\n",
       "0  5.7470   ...      4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n",
       "1  8.0851   ...      7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n",
       "2  5.9525   ...      2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n",
       "3  8.2450   ...      4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n",
       "4  7.6784   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n",
       "\n",
       "   var_196  var_197  var_198  var_199  \n",
       "0   7.8784   8.5635  12.7803  -1.0914  \n",
       "1   8.1267   8.7889  18.3560   1.9518  \n",
       "2  -6.5213   8.2675  14.7222   0.3965  \n",
       "3  -2.9275  10.2922  17.9697  -8.9996  \n",
       "4   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_cv = np.zeros(len(X))\n",
    "pred = np.zeros(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on lgbm\n",
      "fold: 0\n",
      "Training until validation scores don't improve for 800 rounds.\n",
      "[300]\ttraining's auc: 0.763894\tvalid_1's auc: 0.755421\n",
      "[600]\ttraining's auc: 0.803638\tvalid_1's auc: 0.796102\n",
      "[900]\ttraining's auc: 0.824243\tvalid_1's auc: 0.816561\n",
      "[1200]\ttraining's auc: 0.838573\tvalid_1's auc: 0.830135\n",
      "[1500]\ttraining's auc: 0.848948\tvalid_1's auc: 0.839926\n",
      "[1800]\ttraining's auc: 0.856728\tvalid_1's auc: 0.84737\n",
      "[2100]\ttraining's auc: 0.86311\tvalid_1's auc: 0.853287\n",
      "[2400]\ttraining's auc: 0.868888\tvalid_1's auc: 0.858499\n",
      "[2700]\ttraining's auc: 0.873314\tvalid_1's auc: 0.862411\n",
      "[3000]\ttraining's auc: 0.877112\tvalid_1's auc: 0.865793\n",
      "[3300]\ttraining's auc: 0.880634\tvalid_1's auc: 0.868908\n",
      "[3600]\ttraining's auc: 0.883577\tvalid_1's auc: 0.871514\n",
      "[3900]\ttraining's auc: 0.886398\tvalid_1's auc: 0.873991\n",
      "[4200]\ttraining's auc: 0.888777\tvalid_1's auc: 0.876039\n",
      "[4500]\ttraining's auc: 0.890779\tvalid_1's auc: 0.877823\n",
      "[4800]\ttraining's auc: 0.892801\tvalid_1's auc: 0.879531\n",
      "[5100]\ttraining's auc: 0.894456\tvalid_1's auc: 0.880839\n",
      "[5400]\ttraining's auc: 0.895965\tvalid_1's auc: 0.88225\n",
      "[5700]\ttraining's auc: 0.897502\tvalid_1's auc: 0.883371\n",
      "[6000]\ttraining's auc: 0.898793\tvalid_1's auc: 0.884368\n",
      "[6300]\ttraining's auc: 0.899958\tvalid_1's auc: 0.885383\n",
      "[6600]\ttraining's auc: 0.901091\tvalid_1's auc: 0.886306\n",
      "[6900]\ttraining's auc: 0.902113\tvalid_1's auc: 0.887198\n",
      "[7200]\ttraining's auc: 0.903105\tvalid_1's auc: 0.887972\n",
      "[7500]\ttraining's auc: 0.903963\tvalid_1's auc: 0.888676\n",
      "[7800]\ttraining's auc: 0.904879\tvalid_1's auc: 0.889323\n",
      "[8100]\ttraining's auc: 0.90566\tvalid_1's auc: 0.89\n",
      "[8400]\ttraining's auc: 0.906389\tvalid_1's auc: 0.890511\n",
      "[8700]\ttraining's auc: 0.907074\tvalid_1's auc: 0.891037\n",
      "[9000]\ttraining's auc: 0.907816\tvalid_1's auc: 0.891551\n",
      "[9300]\ttraining's auc: 0.908449\tvalid_1's auc: 0.891994\n",
      "[9600]\ttraining's auc: 0.909063\tvalid_1's auc: 0.892498\n",
      "[9900]\ttraining's auc: 0.909629\tvalid_1's auc: 0.892931\n",
      "[10200]\ttraining's auc: 0.910193\tvalid_1's auc: 0.89329\n",
      "[10500]\ttraining's auc: 0.910689\tvalid_1's auc: 0.893754\n",
      "[10800]\ttraining's auc: 0.911198\tvalid_1's auc: 0.894166\n",
      "[11100]\ttraining's auc: 0.911635\tvalid_1's auc: 0.894537\n",
      "[11400]\ttraining's auc: 0.91206\tvalid_1's auc: 0.89484\n",
      "[11700]\ttraining's auc: 0.912475\tvalid_1's auc: 0.895145\n",
      "[12000]\ttraining's auc: 0.912898\tvalid_1's auc: 0.895431\n",
      "[12300]\ttraining's auc: 0.913248\tvalid_1's auc: 0.895653\n",
      "[12600]\ttraining's auc: 0.913614\tvalid_1's auc: 0.895855\n",
      "[12900]\ttraining's auc: 0.913968\tvalid_1's auc: 0.896105\n",
      "[13200]\ttraining's auc: 0.914322\tvalid_1's auc: 0.89626\n",
      "[13500]\ttraining's auc: 0.914615\tvalid_1's auc: 0.896465\n",
      "[13800]\ttraining's auc: 0.914946\tvalid_1's auc: 0.896741\n",
      "[14100]\ttraining's auc: 0.915258\tvalid_1's auc: 0.896907\n",
      "[14400]\ttraining's auc: 0.915536\tvalid_1's auc: 0.897046\n",
      "[14700]\ttraining's auc: 0.915813\tvalid_1's auc: 0.897184\n",
      "[15000]\ttraining's auc: 0.91607\tvalid_1's auc: 0.897342\n",
      "[15300]\ttraining's auc: 0.916319\tvalid_1's auc: 0.897453\n",
      "[15600]\ttraining's auc: 0.916604\tvalid_1's auc: 0.897635\n",
      "[15900]\ttraining's auc: 0.916842\tvalid_1's auc: 0.897759\n",
      "[16200]\ttraining's auc: 0.917102\tvalid_1's auc: 0.897894\n",
      "[16500]\ttraining's auc: 0.917345\tvalid_1's auc: 0.898055\n",
      "[16800]\ttraining's auc: 0.917581\tvalid_1's auc: 0.898181\n",
      "[17100]\ttraining's auc: 0.917784\tvalid_1's auc: 0.898277\n",
      "[17400]\ttraining's auc: 0.917977\tvalid_1's auc: 0.898398\n",
      "[17700]\ttraining's auc: 0.918186\tvalid_1's auc: 0.898452\n",
      "[18000]\ttraining's auc: 0.918426\tvalid_1's auc: 0.898495\n",
      "[18300]\ttraining's auc: 0.918625\tvalid_1's auc: 0.898573\n",
      "[18600]\ttraining's auc: 0.918838\tvalid_1's auc: 0.898644\n",
      "[18900]\ttraining's auc: 0.919045\tvalid_1's auc: 0.898729\n",
      "[19200]\ttraining's auc: 0.919259\tvalid_1's auc: 0.898773\n",
      "[19500]\ttraining's auc: 0.919455\tvalid_1's auc: 0.898875\n",
      "[19800]\ttraining's auc: 0.919676\tvalid_1's auc: 0.898942\n",
      "[20100]\ttraining's auc: 0.919874\tvalid_1's auc: 0.898983\n",
      "[20400]\ttraining's auc: 0.920064\tvalid_1's auc: 0.899017\n",
      "[20700]\ttraining's auc: 0.920269\tvalid_1's auc: 0.8991\n",
      "[21000]\ttraining's auc: 0.920464\tvalid_1's auc: 0.899127\n",
      "[21300]\ttraining's auc: 0.92065\tvalid_1's auc: 0.899146\n",
      "[21600]\ttraining's auc: 0.920821\tvalid_1's auc: 0.899192\n",
      "[21900]\ttraining's auc: 0.921015\tvalid_1's auc: 0.899209\n",
      "[22200]\ttraining's auc: 0.921186\tvalid_1's auc: 0.899245\n",
      "[22500]\ttraining's auc: 0.921364\tvalid_1's auc: 0.899268\n",
      "[22800]\ttraining's auc: 0.921525\tvalid_1's auc: 0.899275\n",
      "[23100]\ttraining's auc: 0.921683\tvalid_1's auc: 0.8993\n",
      "[23400]\ttraining's auc: 0.921851\tvalid_1's auc: 0.899309\n",
      "[23700]\ttraining's auc: 0.922028\tvalid_1's auc: 0.899313\n",
      "[24000]\ttraining's auc: 0.922195\tvalid_1's auc: 0.899318\n",
      "[24300]\ttraining's auc: 0.92235\tvalid_1's auc: 0.899333\n",
      "[24600]\ttraining's auc: 0.92251\tvalid_1's auc: 0.899368\n",
      "[24900]\ttraining's auc: 0.922691\tvalid_1's auc: 0.899355\n",
      "[25200]\ttraining's auc: 0.922858\tvalid_1's auc: 0.899368\n",
      "[25500]\ttraining's auc: 0.923028\tvalid_1's auc: 0.899375\n",
      "[25800]\ttraining's auc: 0.923197\tvalid_1's auc: 0.899381\n",
      "[26100]\ttraining's auc: 0.923356\tvalid_1's auc: 0.899393\n",
      "[26400]\ttraining's auc: 0.923513\tvalid_1's auc: 0.899394\n",
      "[26700]\ttraining's auc: 0.923669\tvalid_1's auc: 0.899401\n",
      "[27000]\ttraining's auc: 0.923833\tvalid_1's auc: 0.899426\n",
      "[27300]\ttraining's auc: 0.923982\tvalid_1's auc: 0.899428\n",
      "[27600]\ttraining's auc: 0.924129\tvalid_1's auc: 0.89946\n",
      "[27900]\ttraining's auc: 0.924287\tvalid_1's auc: 0.899465\n",
      "[28200]\ttraining's auc: 0.924447\tvalid_1's auc: 0.899474\n",
      "[28500]\ttraining's auc: 0.924609\tvalid_1's auc: 0.899475\n",
      "[28800]\ttraining's auc: 0.924767\tvalid_1's auc: 0.89945\n",
      "Early stopping, best iteration is:\n",
      "[28162]\ttraining's auc: 0.924433\tvalid_1's auc: 0.899493\n",
      "fold: 1\n",
      "Training until validation scores don't improve for 800 rounds.\n",
      "[300]\ttraining's auc: 0.76554\tvalid_1's auc: 0.760346\n",
      "[600]\ttraining's auc: 0.805217\tvalid_1's auc: 0.798916\n",
      "[900]\ttraining's auc: 0.824349\tvalid_1's auc: 0.817267\n",
      "[1200]\ttraining's auc: 0.83856\tvalid_1's auc: 0.831073\n",
      "[1500]\ttraining's auc: 0.849241\tvalid_1's auc: 0.841532\n",
      "[1800]\ttraining's auc: 0.856864\tvalid_1's auc: 0.84865\n",
      "[2100]\ttraining's auc: 0.863412\tvalid_1's auc: 0.854863\n",
      "[2400]\ttraining's auc: 0.868655\tvalid_1's auc: 0.859624\n",
      "[2700]\ttraining's auc: 0.87343\tvalid_1's auc: 0.864067\n",
      "[3000]\ttraining's auc: 0.877026\tvalid_1's auc: 0.867164\n",
      "[3300]\ttraining's auc: 0.880458\tvalid_1's auc: 0.870295\n",
      "[3600]\ttraining's auc: 0.883628\tvalid_1's auc: 0.873114\n",
      "[3900]\ttraining's auc: 0.886224\tvalid_1's auc: 0.87535\n",
      "[4200]\ttraining's auc: 0.888446\tvalid_1's auc: 0.877382\n",
      "[4500]\ttraining's auc: 0.89066\tvalid_1's auc: 0.879296\n",
      "[4800]\ttraining's auc: 0.892486\tvalid_1's auc: 0.88079\n",
      "[5100]\ttraining's auc: 0.894119\tvalid_1's auc: 0.882182\n",
      "[5400]\ttraining's auc: 0.895707\tvalid_1's auc: 0.883395\n",
      "[5700]\ttraining's auc: 0.897088\tvalid_1's auc: 0.884564\n",
      "[6000]\ttraining's auc: 0.898408\tvalid_1's auc: 0.885593\n",
      "[6300]\ttraining's auc: 0.899714\tvalid_1's auc: 0.886564\n",
      "[6600]\ttraining's auc: 0.900936\tvalid_1's auc: 0.887608\n",
      "[6900]\ttraining's auc: 0.901937\tvalid_1's auc: 0.888446\n",
      "[7200]\ttraining's auc: 0.902945\tvalid_1's auc: 0.889265\n",
      "[7500]\ttraining's auc: 0.903834\tvalid_1's auc: 0.889948\n",
      "[7800]\ttraining's auc: 0.904683\tvalid_1's auc: 0.890574\n",
      "[8100]\ttraining's auc: 0.905585\tvalid_1's auc: 0.891171\n",
      "[8400]\ttraining's auc: 0.906311\tvalid_1's auc: 0.891687\n",
      "[8700]\ttraining's auc: 0.907043\tvalid_1's auc: 0.892249\n",
      "[9000]\ttraining's auc: 0.907716\tvalid_1's auc: 0.892744\n",
      "[9300]\ttraining's auc: 0.908314\tvalid_1's auc: 0.893168\n",
      "[9600]\ttraining's auc: 0.908934\tvalid_1's auc: 0.893686\n",
      "[9900]\ttraining's auc: 0.909535\tvalid_1's auc: 0.894089\n",
      "[10200]\ttraining's auc: 0.910111\tvalid_1's auc: 0.894472\n",
      "[10500]\ttraining's auc: 0.910542\tvalid_1's auc: 0.894754\n",
      "[10800]\ttraining's auc: 0.91101\tvalid_1's auc: 0.895149\n",
      "[11100]\ttraining's auc: 0.911476\tvalid_1's auc: 0.895459\n",
      "[11400]\ttraining's auc: 0.91193\tvalid_1's auc: 0.895748\n",
      "[11700]\ttraining's auc: 0.912357\tvalid_1's auc: 0.896058\n",
      "[12000]\ttraining's auc: 0.912808\tvalid_1's auc: 0.896327\n",
      "[12300]\ttraining's auc: 0.91318\tvalid_1's auc: 0.896619\n",
      "[12600]\ttraining's auc: 0.913507\tvalid_1's auc: 0.896847\n",
      "[12900]\ttraining's auc: 0.913835\tvalid_1's auc: 0.897064\n",
      "[13200]\ttraining's auc: 0.914133\tvalid_1's auc: 0.897229\n",
      "[13500]\ttraining's auc: 0.914454\tvalid_1's auc: 0.897468\n",
      "[13800]\ttraining's auc: 0.914783\tvalid_1's auc: 0.897688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14100]\ttraining's auc: 0.915085\tvalid_1's auc: 0.897843\n",
      "[14400]\ttraining's auc: 0.915357\tvalid_1's auc: 0.898004\n",
      "[14700]\ttraining's auc: 0.915583\tvalid_1's auc: 0.898145\n",
      "[15000]\ttraining's auc: 0.915868\tvalid_1's auc: 0.898274\n",
      "[15300]\ttraining's auc: 0.916147\tvalid_1's auc: 0.898473\n",
      "[15600]\ttraining's auc: 0.916368\tvalid_1's auc: 0.898552\n",
      "[15900]\ttraining's auc: 0.916608\tvalid_1's auc: 0.898611\n",
      "[16200]\ttraining's auc: 0.916838\tvalid_1's auc: 0.898739\n",
      "[16500]\ttraining's auc: 0.917066\tvalid_1's auc: 0.898843\n",
      "[16800]\ttraining's auc: 0.917311\tvalid_1's auc: 0.898919\n",
      "[17100]\ttraining's auc: 0.917533\tvalid_1's auc: 0.899009\n",
      "[17400]\ttraining's auc: 0.917778\tvalid_1's auc: 0.899074\n",
      "[17700]\ttraining's auc: 0.917991\tvalid_1's auc: 0.899171\n",
      "[18000]\ttraining's auc: 0.918199\tvalid_1's auc: 0.899244\n",
      "[18300]\ttraining's auc: 0.918417\tvalid_1's auc: 0.899285\n",
      "[18600]\ttraining's auc: 0.918618\tvalid_1's auc: 0.899348\n",
      "[18900]\ttraining's auc: 0.918817\tvalid_1's auc: 0.899397\n",
      "[19200]\ttraining's auc: 0.919006\tvalid_1's auc: 0.899461\n",
      "[19500]\ttraining's auc: 0.919203\tvalid_1's auc: 0.89949\n",
      "[19800]\ttraining's auc: 0.9194\tvalid_1's auc: 0.899543\n",
      "[20100]\ttraining's auc: 0.919611\tvalid_1's auc: 0.89962\n",
      "[20400]\ttraining's auc: 0.919791\tvalid_1's auc: 0.899636\n",
      "[20700]\ttraining's auc: 0.91997\tvalid_1's auc: 0.899692\n",
      "[21000]\ttraining's auc: 0.920149\tvalid_1's auc: 0.899712\n",
      "[21300]\ttraining's auc: 0.920326\tvalid_1's auc: 0.899759\n",
      "[21600]\ttraining's auc: 0.920499\tvalid_1's auc: 0.899783\n",
      "[21900]\ttraining's auc: 0.920675\tvalid_1's auc: 0.899784\n",
      "[22200]\ttraining's auc: 0.920836\tvalid_1's auc: 0.899787\n",
      "[22500]\ttraining's auc: 0.921011\tvalid_1's auc: 0.899809\n",
      "[22800]\ttraining's auc: 0.921188\tvalid_1's auc: 0.899818\n",
      "[23100]\ttraining's auc: 0.921366\tvalid_1's auc: 0.899881\n",
      "[23400]\ttraining's auc: 0.921543\tvalid_1's auc: 0.899888\n",
      "[23700]\ttraining's auc: 0.921706\tvalid_1's auc: 0.899903\n",
      "[24000]\ttraining's auc: 0.921873\tvalid_1's auc: 0.899928\n",
      "[24300]\ttraining's auc: 0.922045\tvalid_1's auc: 0.899914\n",
      "[24600]\ttraining's auc: 0.922199\tvalid_1's auc: 0.899981\n",
      "[24900]\ttraining's auc: 0.922366\tvalid_1's auc: 0.899997\n",
      "[25200]\ttraining's auc: 0.922518\tvalid_1's auc: 0.90005\n",
      "[25500]\ttraining's auc: 0.922677\tvalid_1's auc: 0.900048\n",
      "[25800]\ttraining's auc: 0.922824\tvalid_1's auc: 0.90002\n",
      "[26100]\ttraining's auc: 0.922971\tvalid_1's auc: 0.900054\n",
      "[26400]\ttraining's auc: 0.923118\tvalid_1's auc: 0.900079\n",
      "[26700]\ttraining's auc: 0.923282\tvalid_1's auc: 0.900093\n",
      "[27000]\ttraining's auc: 0.923448\tvalid_1's auc: 0.900104\n",
      "[27300]\ttraining's auc: 0.923614\tvalid_1's auc: 0.90009\n",
      "[27600]\ttraining's auc: 0.923773\tvalid_1's auc: 0.900108\n",
      "[27900]\ttraining's auc: 0.92392\tvalid_1's auc: 0.900118\n",
      "[28200]\ttraining's auc: 0.92406\tvalid_1's auc: 0.90015\n",
      "[28500]\ttraining's auc: 0.924218\tvalid_1's auc: 0.900159\n",
      "[28800]\ttraining's auc: 0.92438\tvalid_1's auc: 0.90013\n",
      "[29100]\ttraining's auc: 0.924537\tvalid_1's auc: 0.900109\n",
      "Early stopping, best iteration is:\n",
      "[28398]\ttraining's auc: 0.924163\tvalid_1's auc: 0.90017\n",
      "fold: 2\n",
      "Training until validation scores don't improve for 800 rounds.\n",
      "[300]\ttraining's auc: 0.765573\tvalid_1's auc: 0.760692\n",
      "[600]\ttraining's auc: 0.803731\tvalid_1's auc: 0.797991\n",
      "[900]\ttraining's auc: 0.823822\tvalid_1's auc: 0.817777\n",
      "[1200]\ttraining's auc: 0.837484\tvalid_1's auc: 0.831526\n",
      "[1500]\ttraining's auc: 0.848048\tvalid_1's auc: 0.841769\n",
      "[1800]\ttraining's auc: 0.856094\tvalid_1's auc: 0.849364\n",
      "[2100]\ttraining's auc: 0.862061\tvalid_1's auc: 0.855282\n",
      "[2400]\ttraining's auc: 0.867784\tvalid_1's auc: 0.860989\n",
      "[2700]\ttraining's auc: 0.872328\tvalid_1's auc: 0.865344\n",
      "[3000]\ttraining's auc: 0.875928\tvalid_1's auc: 0.868714\n",
      "[3300]\ttraining's auc: 0.879216\tvalid_1's auc: 0.872059\n",
      "[3600]\ttraining's auc: 0.882037\tvalid_1's auc: 0.874783\n",
      "[3900]\ttraining's auc: 0.884741\tvalid_1's auc: 0.877451\n",
      "[4200]\ttraining's auc: 0.887132\tvalid_1's auc: 0.879865\n",
      "[4500]\ttraining's auc: 0.889214\tvalid_1's auc: 0.881898\n",
      "[4800]\ttraining's auc: 0.891212\tvalid_1's auc: 0.88369\n",
      "[5100]\ttraining's auc: 0.892937\tvalid_1's auc: 0.885311\n",
      "[5400]\ttraining's auc: 0.894474\tvalid_1's auc: 0.88681\n",
      "[5700]\ttraining's auc: 0.895795\tvalid_1's auc: 0.888025\n",
      "[6000]\ttraining's auc: 0.897082\tvalid_1's auc: 0.889185\n",
      "[6300]\ttraining's auc: 0.898243\tvalid_1's auc: 0.890369\n",
      "[6600]\ttraining's auc: 0.899458\tvalid_1's auc: 0.891368\n",
      "[6900]\ttraining's auc: 0.900484\tvalid_1's auc: 0.892292\n",
      "[7200]\ttraining's auc: 0.901403\tvalid_1's auc: 0.8931\n",
      "[7500]\ttraining's auc: 0.902293\tvalid_1's auc: 0.893943\n",
      "[7800]\ttraining's auc: 0.903166\tvalid_1's auc: 0.894638\n",
      "[8100]\ttraining's auc: 0.903955\tvalid_1's auc: 0.895379\n",
      "[8400]\ttraining's auc: 0.904748\tvalid_1's auc: 0.896102\n",
      "[8700]\ttraining's auc: 0.905449\tvalid_1's auc: 0.89671\n",
      "[9000]\ttraining's auc: 0.906134\tvalid_1's auc: 0.897293\n",
      "[9300]\ttraining's auc: 0.906814\tvalid_1's auc: 0.897854\n",
      "[9600]\ttraining's auc: 0.907397\tvalid_1's auc: 0.89828\n",
      "[9900]\ttraining's auc: 0.907914\tvalid_1's auc: 0.898688\n",
      "[10200]\ttraining's auc: 0.908489\tvalid_1's auc: 0.899193\n",
      "[10500]\ttraining's auc: 0.909019\tvalid_1's auc: 0.899588\n",
      "[10800]\ttraining's auc: 0.909469\tvalid_1's auc: 0.900007\n",
      "[11100]\ttraining's auc: 0.90995\tvalid_1's auc: 0.900342\n",
      "[11400]\ttraining's auc: 0.91037\tvalid_1's auc: 0.900667\n",
      "[11700]\ttraining's auc: 0.91079\tvalid_1's auc: 0.901001\n",
      "[12000]\ttraining's auc: 0.911215\tvalid_1's auc: 0.9013\n",
      "[12300]\ttraining's auc: 0.911599\tvalid_1's auc: 0.90164\n",
      "[12600]\ttraining's auc: 0.911943\tvalid_1's auc: 0.901882\n",
      "[12900]\ttraining's auc: 0.912273\tvalid_1's auc: 0.902105\n",
      "[13200]\ttraining's auc: 0.912632\tvalid_1's auc: 0.902366\n",
      "[13500]\ttraining's auc: 0.912938\tvalid_1's auc: 0.902575\n",
      "[13800]\ttraining's auc: 0.913238\tvalid_1's auc: 0.902776\n",
      "[14100]\ttraining's auc: 0.91354\tvalid_1's auc: 0.90298\n",
      "[14400]\ttraining's auc: 0.913848\tvalid_1's auc: 0.903181\n",
      "[14700]\ttraining's auc: 0.914131\tvalid_1's auc: 0.903347\n",
      "[15000]\ttraining's auc: 0.914418\tvalid_1's auc: 0.903525\n",
      "[15300]\ttraining's auc: 0.914681\tvalid_1's auc: 0.903701\n",
      "[15600]\ttraining's auc: 0.914939\tvalid_1's auc: 0.903885\n",
      "[15900]\ttraining's auc: 0.915187\tvalid_1's auc: 0.903944\n",
      "[16200]\ttraining's auc: 0.915431\tvalid_1's auc: 0.904018\n",
      "[16500]\ttraining's auc: 0.915694\tvalid_1's auc: 0.904184\n",
      "[16800]\ttraining's auc: 0.915913\tvalid_1's auc: 0.904272\n",
      "[17100]\ttraining's auc: 0.916158\tvalid_1's auc: 0.904373\n",
      "[17400]\ttraining's auc: 0.916366\tvalid_1's auc: 0.904461\n",
      "[17700]\ttraining's auc: 0.916567\tvalid_1's auc: 0.904594\n",
      "[18000]\ttraining's auc: 0.916777\tvalid_1's auc: 0.904649\n",
      "[18300]\ttraining's auc: 0.916994\tvalid_1's auc: 0.904734\n",
      "[18600]\ttraining's auc: 0.917192\tvalid_1's auc: 0.904834\n",
      "[18900]\ttraining's auc: 0.917379\tvalid_1's auc: 0.904924\n",
      "[19200]\ttraining's auc: 0.917602\tvalid_1's auc: 0.905016\n",
      "[19500]\ttraining's auc: 0.917812\tvalid_1's auc: 0.905122\n",
      "[19800]\ttraining's auc: 0.918017\tvalid_1's auc: 0.905218\n",
      "[20100]\ttraining's auc: 0.918193\tvalid_1's auc: 0.905305\n",
      "[20400]\ttraining's auc: 0.918365\tvalid_1's auc: 0.905298\n",
      "[20700]\ttraining's auc: 0.918573\tvalid_1's auc: 0.905347\n",
      "[21000]\ttraining's auc: 0.918743\tvalid_1's auc: 0.905381\n",
      "[21300]\ttraining's auc: 0.918922\tvalid_1's auc: 0.905434\n",
      "[21600]\ttraining's auc: 0.919099\tvalid_1's auc: 0.905466\n",
      "[21900]\ttraining's auc: 0.919293\tvalid_1's auc: 0.905548\n",
      "[22200]\ttraining's auc: 0.919468\tvalid_1's auc: 0.905611\n",
      "[22500]\ttraining's auc: 0.919635\tvalid_1's auc: 0.905654\n",
      "[22800]\ttraining's auc: 0.919818\tvalid_1's auc: 0.905693\n",
      "[23100]\ttraining's auc: 0.92\tvalid_1's auc: 0.905719\n",
      "[23400]\ttraining's auc: 0.920172\tvalid_1's auc: 0.905759\n",
      "[23700]\ttraining's auc: 0.92033\tvalid_1's auc: 0.905803\n",
      "[24000]\ttraining's auc: 0.920507\tvalid_1's auc: 0.905835\n",
      "[24300]\ttraining's auc: 0.920662\tvalid_1's auc: 0.905833\n",
      "[24600]\ttraining's auc: 0.920844\tvalid_1's auc: 0.905874\n",
      "[24900]\ttraining's auc: 0.921002\tvalid_1's auc: 0.905885\n",
      "[25200]\ttraining's auc: 0.921166\tvalid_1's auc: 0.905885\n",
      "[25500]\ttraining's auc: 0.921322\tvalid_1's auc: 0.905876\n",
      "[25800]\ttraining's auc: 0.921477\tvalid_1's auc: 0.905895\n",
      "[26100]\ttraining's auc: 0.921641\tvalid_1's auc: 0.905866\n",
      "[26400]\ttraining's auc: 0.921822\tvalid_1's auc: 0.905894\n",
      "[26700]\ttraining's auc: 0.921989\tvalid_1's auc: 0.905923\n",
      "[27000]\ttraining's auc: 0.922145\tvalid_1's auc: 0.905911\n",
      "[27300]\ttraining's auc: 0.922325\tvalid_1's auc: 0.905947\n",
      "[27600]\ttraining's auc: 0.92249\tvalid_1's auc: 0.905967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27900]\ttraining's auc: 0.922651\tvalid_1's auc: 0.905958\n",
      "[28200]\ttraining's auc: 0.922798\tvalid_1's auc: 0.905933\n",
      "Early stopping, best iteration is:\n",
      "[27518]\ttraining's auc: 0.922452\tvalid_1's auc: 0.905972\n",
      "fold: 3\n",
      "Training until validation scores don't improve for 800 rounds.\n",
      "[300]\ttraining's auc: 0.767513\tvalid_1's auc: 0.751512\n",
      "[600]\ttraining's auc: 0.803304\tvalid_1's auc: 0.788101\n",
      "[900]\ttraining's auc: 0.823503\tvalid_1's auc: 0.808762\n",
      "[1200]\ttraining's auc: 0.837394\tvalid_1's auc: 0.822844\n",
      "[1500]\ttraining's auc: 0.84779\tvalid_1's auc: 0.833545\n",
      "[1800]\ttraining's auc: 0.856382\tvalid_1's auc: 0.842137\n",
      "[2100]\ttraining's auc: 0.863134\tvalid_1's auc: 0.84884\n",
      "[2400]\ttraining's auc: 0.868402\tvalid_1's auc: 0.854127\n",
      "[2700]\ttraining's auc: 0.872846\tvalid_1's auc: 0.858633\n",
      "[3000]\ttraining's auc: 0.8768\tvalid_1's auc: 0.862373\n",
      "[3300]\ttraining's auc: 0.880068\tvalid_1's auc: 0.865526\n",
      "[3600]\ttraining's auc: 0.883233\tvalid_1's auc: 0.868625\n",
      "[3900]\ttraining's auc: 0.885983\tvalid_1's auc: 0.871205\n",
      "[4200]\ttraining's auc: 0.888414\tvalid_1's auc: 0.873495\n",
      "[4500]\ttraining's auc: 0.890446\tvalid_1's auc: 0.875221\n",
      "[4800]\ttraining's auc: 0.892437\tvalid_1's auc: 0.877051\n",
      "[5100]\ttraining's auc: 0.894235\tvalid_1's auc: 0.878504\n",
      "[5400]\ttraining's auc: 0.895827\tvalid_1's auc: 0.880015\n",
      "[5700]\ttraining's auc: 0.897333\tvalid_1's auc: 0.881272\n",
      "[6000]\ttraining's auc: 0.898709\tvalid_1's auc: 0.8824\n",
      "[6300]\ttraining's auc: 0.899866\tvalid_1's auc: 0.883302\n",
      "[6600]\ttraining's auc: 0.900987\tvalid_1's auc: 0.8842\n",
      "[6900]\ttraining's auc: 0.902207\tvalid_1's auc: 0.885252\n",
      "[7200]\ttraining's auc: 0.903186\tvalid_1's auc: 0.886008\n",
      "[7500]\ttraining's auc: 0.904131\tvalid_1's auc: 0.886825\n",
      "[7800]\ttraining's auc: 0.90503\tvalid_1's auc: 0.887482\n",
      "[8100]\ttraining's auc: 0.905831\tvalid_1's auc: 0.888146\n",
      "[8400]\ttraining's auc: 0.906591\tvalid_1's auc: 0.888758\n",
      "[8700]\ttraining's auc: 0.907331\tvalid_1's auc: 0.889262\n",
      "[9000]\ttraining's auc: 0.908092\tvalid_1's auc: 0.889864\n",
      "[9300]\ttraining's auc: 0.908657\tvalid_1's auc: 0.890303\n",
      "[9600]\ttraining's auc: 0.909318\tvalid_1's auc: 0.89082\n",
      "[9900]\ttraining's auc: 0.909877\tvalid_1's auc: 0.891211\n",
      "[10200]\ttraining's auc: 0.910414\tvalid_1's auc: 0.891571\n",
      "[10500]\ttraining's auc: 0.910929\tvalid_1's auc: 0.891869\n",
      "[10800]\ttraining's auc: 0.911404\tvalid_1's auc: 0.892252\n",
      "[11100]\ttraining's auc: 0.911866\tvalid_1's auc: 0.892669\n",
      "[11400]\ttraining's auc: 0.912298\tvalid_1's auc: 0.893043\n",
      "[11700]\ttraining's auc: 0.912758\tvalid_1's auc: 0.893405\n",
      "[12000]\ttraining's auc: 0.913112\tvalid_1's auc: 0.893618\n",
      "[12300]\ttraining's auc: 0.913514\tvalid_1's auc: 0.893901\n",
      "[12600]\ttraining's auc: 0.913889\tvalid_1's auc: 0.894209\n",
      "[12900]\ttraining's auc: 0.914215\tvalid_1's auc: 0.894456\n",
      "[13200]\ttraining's auc: 0.91455\tvalid_1's auc: 0.894693\n",
      "[13500]\ttraining's auc: 0.914845\tvalid_1's auc: 0.894907\n",
      "[13800]\ttraining's auc: 0.915165\tvalid_1's auc: 0.895056\n",
      "[14100]\ttraining's auc: 0.915431\tvalid_1's auc: 0.895213\n",
      "[14400]\ttraining's auc: 0.915738\tvalid_1's auc: 0.89541\n",
      "[14700]\ttraining's auc: 0.915975\tvalid_1's auc: 0.895538\n",
      "[15000]\ttraining's auc: 0.916247\tvalid_1's auc: 0.895742\n",
      "[15300]\ttraining's auc: 0.916497\tvalid_1's auc: 0.895921\n",
      "[15600]\ttraining's auc: 0.916721\tvalid_1's auc: 0.896061\n",
      "[15900]\ttraining's auc: 0.916971\tvalid_1's auc: 0.896231\n",
      "[16200]\ttraining's auc: 0.917172\tvalid_1's auc: 0.896346\n",
      "[16500]\ttraining's auc: 0.917425\tvalid_1's auc: 0.896509\n",
      "[16800]\ttraining's auc: 0.917656\tvalid_1's auc: 0.896683\n",
      "[17100]\ttraining's auc: 0.917886\tvalid_1's auc: 0.89685\n",
      "[17400]\ttraining's auc: 0.918117\tvalid_1's auc: 0.896968\n",
      "[17700]\ttraining's auc: 0.918339\tvalid_1's auc: 0.897059\n",
      "[18000]\ttraining's auc: 0.91856\tvalid_1's auc: 0.897126\n",
      "[18300]\ttraining's auc: 0.918737\tvalid_1's auc: 0.897211\n",
      "[18600]\ttraining's auc: 0.918956\tvalid_1's auc: 0.897295\n",
      "[18900]\ttraining's auc: 0.919164\tvalid_1's auc: 0.897384\n",
      "[19200]\ttraining's auc: 0.919359\tvalid_1's auc: 0.897433\n",
      "[19500]\ttraining's auc: 0.919566\tvalid_1's auc: 0.897567\n",
      "[19800]\ttraining's auc: 0.919749\tvalid_1's auc: 0.897679\n",
      "[20100]\ttraining's auc: 0.919943\tvalid_1's auc: 0.897747\n",
      "[20400]\ttraining's auc: 0.920132\tvalid_1's auc: 0.897826\n",
      "[20700]\ttraining's auc: 0.920319\tvalid_1's auc: 0.897889\n",
      "[21000]\ttraining's auc: 0.920488\tvalid_1's auc: 0.897947\n",
      "[21300]\ttraining's auc: 0.920676\tvalid_1's auc: 0.89801\n",
      "[21600]\ttraining's auc: 0.920853\tvalid_1's auc: 0.898042\n",
      "[21900]\ttraining's auc: 0.921016\tvalid_1's auc: 0.898117\n",
      "[22200]\ttraining's auc: 0.921194\tvalid_1's auc: 0.898134\n",
      "[22500]\ttraining's auc: 0.921377\tvalid_1's auc: 0.898197\n",
      "[22800]\ttraining's auc: 0.921558\tvalid_1's auc: 0.898237\n",
      "[23100]\ttraining's auc: 0.921733\tvalid_1's auc: 0.898265\n",
      "[23400]\ttraining's auc: 0.921892\tvalid_1's auc: 0.898295\n",
      "[23700]\ttraining's auc: 0.922064\tvalid_1's auc: 0.898323\n",
      "[24000]\ttraining's auc: 0.922244\tvalid_1's auc: 0.898365\n",
      "[24300]\ttraining's auc: 0.922426\tvalid_1's auc: 0.89842\n",
      "[24600]\ttraining's auc: 0.922576\tvalid_1's auc: 0.898455\n",
      "[24900]\ttraining's auc: 0.922721\tvalid_1's auc: 0.898491\n",
      "[25200]\ttraining's auc: 0.922907\tvalid_1's auc: 0.898513\n",
      "[25500]\ttraining's auc: 0.923078\tvalid_1's auc: 0.898543\n",
      "[25800]\ttraining's auc: 0.923219\tvalid_1's auc: 0.898566\n",
      "[26100]\ttraining's auc: 0.923383\tvalid_1's auc: 0.89858\n",
      "[26400]\ttraining's auc: 0.923552\tvalid_1's auc: 0.898569\n",
      "[26700]\ttraining's auc: 0.923721\tvalid_1's auc: 0.898574\n",
      "Early stopping, best iteration is:\n",
      "[26011]\ttraining's auc: 0.923336\tvalid_1's auc: 0.898595\n",
      "fold: 4\n",
      "Training until validation scores don't improve for 800 rounds.\n",
      "[300]\ttraining's auc: 0.768573\tvalid_1's auc: 0.762674\n",
      "[600]\ttraining's auc: 0.805117\tvalid_1's auc: 0.796007\n",
      "[900]\ttraining's auc: 0.826114\tvalid_1's auc: 0.814562\n",
      "[1200]\ttraining's auc: 0.840403\tvalid_1's auc: 0.827144\n",
      "[1500]\ttraining's auc: 0.850766\tvalid_1's auc: 0.836162\n",
      "[1800]\ttraining's auc: 0.858816\tvalid_1's auc: 0.843063\n",
      "[2100]\ttraining's auc: 0.865147\tvalid_1's auc: 0.848775\n",
      "[2400]\ttraining's auc: 0.870353\tvalid_1's auc: 0.853383\n",
      "[2700]\ttraining's auc: 0.874985\tvalid_1's auc: 0.857597\n",
      "[3000]\ttraining's auc: 0.878715\tvalid_1's auc: 0.860839\n",
      "[3300]\ttraining's auc: 0.882178\tvalid_1's auc: 0.864053\n",
      "[3600]\ttraining's auc: 0.88482\tvalid_1's auc: 0.866302\n",
      "[3900]\ttraining's auc: 0.887301\tvalid_1's auc: 0.86851\n",
      "[4200]\ttraining's auc: 0.889636\tvalid_1's auc: 0.870583\n",
      "[4500]\ttraining's auc: 0.891433\tvalid_1's auc: 0.872447\n",
      "[4800]\ttraining's auc: 0.893409\tvalid_1's auc: 0.8743\n",
      "[5100]\ttraining's auc: 0.895139\tvalid_1's auc: 0.875861\n",
      "[5400]\ttraining's auc: 0.896481\tvalid_1's auc: 0.877232\n",
      "[5700]\ttraining's auc: 0.897967\tvalid_1's auc: 0.878644\n",
      "[6000]\ttraining's auc: 0.89931\tvalid_1's auc: 0.879877\n",
      "[6300]\ttraining's auc: 0.900533\tvalid_1's auc: 0.880986\n",
      "[6600]\ttraining's auc: 0.901688\tvalid_1's auc: 0.882029\n",
      "[6900]\ttraining's auc: 0.902644\tvalid_1's auc: 0.882855\n",
      "[7200]\ttraining's auc: 0.903603\tvalid_1's auc: 0.883759\n",
      "[7500]\ttraining's auc: 0.904553\tvalid_1's auc: 0.884625\n",
      "[7800]\ttraining's auc: 0.905401\tvalid_1's auc: 0.885343\n",
      "[8100]\ttraining's auc: 0.906243\tvalid_1's auc: 0.886113\n",
      "[8400]\ttraining's auc: 0.906916\tvalid_1's auc: 0.886701\n",
      "[8700]\ttraining's auc: 0.907615\tvalid_1's auc: 0.887342\n",
      "[9000]\ttraining's auc: 0.908193\tvalid_1's auc: 0.887839\n",
      "[9300]\ttraining's auc: 0.908878\tvalid_1's auc: 0.88846\n",
      "[9600]\ttraining's auc: 0.909506\tvalid_1's auc: 0.889004\n",
      "[9900]\ttraining's auc: 0.910066\tvalid_1's auc: 0.889461\n",
      "[10200]\ttraining's auc: 0.910598\tvalid_1's auc: 0.889858\n",
      "[10500]\ttraining's auc: 0.911117\tvalid_1's auc: 0.890305\n",
      "[10800]\ttraining's auc: 0.911616\tvalid_1's auc: 0.890773\n",
      "[11100]\ttraining's auc: 0.912058\tvalid_1's auc: 0.891152\n",
      "[11400]\ttraining's auc: 0.912484\tvalid_1's auc: 0.891463\n",
      "[11700]\ttraining's auc: 0.912913\tvalid_1's auc: 0.891814\n",
      "[12000]\ttraining's auc: 0.913316\tvalid_1's auc: 0.892127\n",
      "[12300]\ttraining's auc: 0.913675\tvalid_1's auc: 0.892434\n",
      "[12600]\ttraining's auc: 0.914048\tvalid_1's auc: 0.892641\n",
      "[12900]\ttraining's auc: 0.914398\tvalid_1's auc: 0.892883\n",
      "[13200]\ttraining's auc: 0.914749\tvalid_1's auc: 0.893134\n",
      "[13500]\ttraining's auc: 0.915055\tvalid_1's auc: 0.893366\n",
      "[13800]\ttraining's auc: 0.915392\tvalid_1's auc: 0.893595\n",
      "[14100]\ttraining's auc: 0.915677\tvalid_1's auc: 0.893715\n",
      "[14400]\ttraining's auc: 0.915959\tvalid_1's auc: 0.893931\n",
      "[14700]\ttraining's auc: 0.916211\tvalid_1's auc: 0.894087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15000]\ttraining's auc: 0.916482\tvalid_1's auc: 0.894219\n",
      "[15300]\ttraining's auc: 0.916762\tvalid_1's auc: 0.894449\n",
      "[15600]\ttraining's auc: 0.917024\tvalid_1's auc: 0.894656\n",
      "[15900]\ttraining's auc: 0.917268\tvalid_1's auc: 0.894795\n",
      "[16200]\ttraining's auc: 0.917511\tvalid_1's auc: 0.894931\n",
      "[16500]\ttraining's auc: 0.917752\tvalid_1's auc: 0.895006\n",
      "[16800]\ttraining's auc: 0.917978\tvalid_1's auc: 0.895126\n",
      "[17100]\ttraining's auc: 0.918212\tvalid_1's auc: 0.895219\n",
      "[17400]\ttraining's auc: 0.918425\tvalid_1's auc: 0.89529\n",
      "[17700]\ttraining's auc: 0.918656\tvalid_1's auc: 0.895388\n",
      "[18000]\ttraining's auc: 0.918886\tvalid_1's auc: 0.895496\n",
      "[18300]\ttraining's auc: 0.919081\tvalid_1's auc: 0.895614\n",
      "[18600]\ttraining's auc: 0.919278\tvalid_1's auc: 0.895719\n",
      "[18900]\ttraining's auc: 0.919465\tvalid_1's auc: 0.895796\n",
      "[19200]\ttraining's auc: 0.919659\tvalid_1's auc: 0.89586\n",
      "[19500]\ttraining's auc: 0.919859\tvalid_1's auc: 0.895934\n",
      "[19800]\ttraining's auc: 0.920053\tvalid_1's auc: 0.89598\n",
      "[20100]\ttraining's auc: 0.920233\tvalid_1's auc: 0.896039\n",
      "[20400]\ttraining's auc: 0.920424\tvalid_1's auc: 0.896113\n",
      "[20700]\ttraining's auc: 0.920613\tvalid_1's auc: 0.896101\n",
      "[21000]\ttraining's auc: 0.92081\tvalid_1's auc: 0.89616\n",
      "[21300]\ttraining's auc: 0.92098\tvalid_1's auc: 0.896227\n",
      "[21600]\ttraining's auc: 0.921165\tvalid_1's auc: 0.896269\n",
      "[21900]\ttraining's auc: 0.921319\tvalid_1's auc: 0.896273\n",
      "[22200]\ttraining's auc: 0.921488\tvalid_1's auc: 0.89631\n",
      "[22500]\ttraining's auc: 0.921655\tvalid_1's auc: 0.896394\n",
      "[22800]\ttraining's auc: 0.921834\tvalid_1's auc: 0.896405\n",
      "[23100]\ttraining's auc: 0.922008\tvalid_1's auc: 0.896399\n",
      "[23400]\ttraining's auc: 0.922162\tvalid_1's auc: 0.89646\n",
      "[23700]\ttraining's auc: 0.922329\tvalid_1's auc: 0.896493\n",
      "[24000]\ttraining's auc: 0.922489\tvalid_1's auc: 0.896525\n",
      "[24300]\ttraining's auc: 0.92266\tvalid_1's auc: 0.896538\n",
      "[24600]\ttraining's auc: 0.922818\tvalid_1's auc: 0.896572\n",
      "[24900]\ttraining's auc: 0.922979\tvalid_1's auc: 0.896558\n",
      "[25200]\ttraining's auc: 0.923139\tvalid_1's auc: 0.896578\n",
      "[25500]\ttraining's auc: 0.923302\tvalid_1's auc: 0.896575\n",
      "[25800]\ttraining's auc: 0.923466\tvalid_1's auc: 0.896628\n",
      "[26100]\ttraining's auc: 0.923638\tvalid_1's auc: 0.896615\n",
      "[26400]\ttraining's auc: 0.923795\tvalid_1's auc: 0.896626\n",
      "Early stopping, best iteration is:\n",
      "[25761]\ttraining's auc: 0.923448\tvalid_1's auc: 0.896636\n",
      "[0.00863358 0.32628052 0.00441017 ... 0.05964156 0.04672252 0.00924259]\n",
      "working on lg\n",
      "fold: 0\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This SGDClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-b756cc354cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[0mpred_cv\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1036\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"t_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"log\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: This SGDClassifier instance is not fitted yet. Call 'fit' with appropriate arguments before using this method."
     ]
    }
   ],
   "source": [
    "# stacking\n",
    "\n",
    "lgbm_params = {\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'min_data_in_leaf': 2881,\n",
    "    'max_depth': 0,\n",
    "    'num_leaves': 3,\n",
    "    'learning_rate': 0.01,\n",
    "    'bagging_freq': 3,\n",
    "    #'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.3, 0.9),\n",
    "    'feature_fraction': 0.8453828656355421,\n",
    "    'bagging_seed': 11,\n",
    "    'reg_alpha':  1.1173044727720816,\n",
    "    'reg_lambda': 6.9285776442737514,\n",
    "    'random_state': 42,\n",
    "    'verbosity': -1,\n",
    "    'subsample':0.8421287738494433,\n",
    "    'min_child_weight': 36.93038816860224,\n",
    "    'num_threads': 4,\n",
    "    'max_bin': 483\n",
    "}\n",
    "\n",
    "models = [\n",
    "#    ('lgbm', None),\n",
    "    ('lg', SGDClassifier(loss='log', max_iter=5000, tol=1e-7, alpha=0.3)),\n",
    "    ('mlp', MLPClassifier(solver='lbfgs', alpha=0.001, hidden_layer_sizes=(5, 2), random_state=1)),\n",
    "    ('rf', RandomForestClassifier(n_estimators=10, criterion='entropy')),\n",
    "    ('gnb', GaussianNB()),\n",
    "    ('qda', QuadraticDiscriminantAnalysis(tol=1e-12)),\n",
    "]\n",
    "\n",
    "cv_out = pd.DataFrame(index=train_df.iloc[:,0])\n",
    "test_out = pd.DataFrame(index=test_df.iloc[:,0])\n",
    "\n",
    "# add target to cv_out\n",
    "cv_out['target'] = train_df.iloc[:,1].values\n",
    "\n",
    "for model in models:\n",
    "    name = model[0]\n",
    "    cls = model[1]\n",
    "    print('working on ' + name)\n",
    "    \n",
    "    # k-fold\n",
    "    n_splits = 5\n",
    "    folds = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    pred_cv = np.zeros(len(X))\n",
    "    pred = np.zeros(len(X_test))\n",
    "    \n",
    "    for fold_n, (train_index, valid_index) in enumerate(folds.split(X,y)):\n",
    "        print(\"fold: \" + str(fold_n))\n",
    "        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n",
    "        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n",
    "                        \n",
    "        if name == 'lgbm':\n",
    "            train_data = lgb.Dataset(X_train, label=y_train)\n",
    "            valid_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "            res = lgb.train(lgbm_params,train_data,num_boost_round=2000000,\n",
    "                    valid_sets = [train_data, valid_data],verbose_eval=300,early_stopping_rounds = 800)\n",
    "            pred_cv += res.predict(X, num_iteration=res.best_iteration) / n_splits\n",
    "            pred += res.predict(X_test, num_iteration=res.best_iteration) / n_splits\n",
    "        else:\n",
    "            cls.fit(X_train, y_train)\n",
    "            pred_cv += cls.predict_proba(X)[:,1] / n_splits\n",
    "            pred += cls.predict_proba(X_test)[:,1] / n_splits\n",
    "    \n",
    "    print(pred_cv)\n",
    "        \n",
    "    cv_out[name] = pred_cv\n",
    "    test_out[name] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add statistics\n",
    "num = len(models);\n",
    "\n",
    "cv_out['mean'] = cv_out.iloc[:,[1,num]].mean(axis=1)\n",
    "cv_out['min'] = cv_out.iloc[:,[1,num]].min(axis=1)\n",
    "cv_out['max'] = cv_out.iloc[:,[1,num]].max(axis=1)\n",
    "\n",
    "test_out['mean'] = test_out.iloc[:,[0,num-1]].mean(axis=1)\n",
    "test_out['min'] = test_out.iloc[:,[0,num-1]].min(axis=1)\n",
    "test_out['max'] = test_out.iloc[:,[0,num-1]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_out.to_csv('input/train_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_out.to_csv('input/test_stack.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
